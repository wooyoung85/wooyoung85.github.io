> [ì‹¤ë¬´ í”„ë¡œì íŠ¸ë¡œ ë°°ìš°ëŠ” ë¹…ë°ì´í„° ê¸°ìˆ ](https://wikibook.co.kr/bigdata/) ì„ ì½ê³  ì¢€ ë” ì‹¤ë¬´ í™˜ê²½ì— ê°€ê¹ê²Œ ì‹¤ìŠµí•´ ë³¼ ìˆ˜ ìˆëŠ” ë°©ë²•ì— ëŒ€í•´ ì ì–´ ë³´ì•˜ìŠµë‹ˆë‹¤.

> ë¹…ë°ì´í„° ê¸°ìˆ ì— ê´€ì‹¬ì´ ìˆë‹¤ë©´ [ì‹¤ë¬´ í”„ë¡œì íŠ¸ë¡œ ë°°ìš°ëŠ” ë¹…ë°ì´í„° ê¸°ìˆ ](https://wikibook.co.kr/bigdada/) ì„ ì½ì–´ë³´ì‹œê¸¸ ê¼­ ì¶”ì²œ ë“œë¦½ë‹ˆë‹¤ ^^

# 01. Google Cloud Platformìœ¼ë¡œ ì‘ì—… í™˜ê²½ êµ¬ì„±

- [Google Cloud Platform ì‚¬ìš©í•˜ê¸°](https://wooyoung85.tistory.com/51)
- [Cloudera Manager ì„¤ì¹˜](https://wooyoung85.tistory.com/50)
- host ì„¤ì • (Mac OS ê¸°ì¤€)
  ```
  $> sudo nano /private/etc/hosts
  $> dscacheutil -flushcache
  ```

# 02. ë¹…ë°ì´í„° íŒŒì¼ëŸ¿ í”„ë¡œì íŠ¸

## ë¹…ë°ì´í„° ê¸°ë³¸ ì†Œí”„íŠ¸ì›¨ì–´ ì„¤ì¹˜

- HDFS, YARN(MR2 Included), ì£¼í‚¤í¼ ì„¤ì¹˜
  ![ë¹…ë°ì´í„° ê¸°ë³¸ ì†Œí”„íŠ¸ì›¨ì–´ ì„¤ì¹˜](./images/bigdata_practice/1.PNG)
- ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • - Report Manager

  | Database í˜¸ìŠ¤íŠ¸ ì´ë¦„ | ë°ì´í„°ë² ì´ìŠ¤ ìœ í˜• | ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„ | ì‚¬ìš©ì ì´ë¦„ | ì•”í˜¸ |
  | -------------------- | ----------------- | ----------------- | ----------- | ---- |
  | node1                | MySQL             | rman              | rman        | rman |

## ì‹¤ìŠµ ìë£Œ ë‹¤ìš´ë¡œë“œ

- `node2` ì„œë²„ì— `root`ë¡œ ssh ì ‘ì†
- ì‹¤ìŠµìë£ŒëŠ” `/git/bigdata` ì— ì €ì¥
- ì‘ì—…í´ë”ëŠ” `/home/pilot-pjt` ë¥¼ ì‚¬ìš©í•  ì˜ˆì •

  ```bash
  $> yum install -y git
  $> mkdir git && cd git
  $> git clone https://github.com/wikibook/bigdata

  ```

## Hdfs ëª…ë ¹ì–´ ì‹¤ìŠµ

- `node2` ì„œë²„ì— `root`ë¡œ ssh ì ‘ì†
- Sample.txt íŒŒì¼ ì¤€ë¹„

  ```bash
  $> cd
  $> mkdir -p /home/bigdata
  $> cp /root/git/bigdata/CH02/Sample.txt /home/bigdata/.
  ```

- **Skipí•´ë„ ë¨**

  ```bash
  $> hdfs dfs -put /home/bigdata/Sample.txt /tmp
  $> hdfs dfs -ls /tmp
  $> hdfs dfs -cat /tmp/Sample.txt

  $> hdfs dfs -stat '%b %o %r %u %n' /tmp/Sample.txt
  $> hdfs dfs -mv /tmp/Sample.txt /tmp/Sample2.txt
  $> hdfs fsck
  $> hdfs dfsadmin -report

  $> hdfs dfs -get /tmp/Sample2.txt
  $> sudo -u hdfs hdfs dfs -rm /tmp/Sample2.txt
  ```

## ì£¼í‚¤í¼ ì„¤ì¹˜ í™•ì¸ (ì•ˆí•´ë„ ë¨)

- `node2` ì„œë²„ì— `root`ë¡œ ssh ì ‘ì†
- **Skipí•´ë„ ë¨**

  ```bash
  $> zookeeper-client

  # create [ë…¸ë“œ ê²½ë¡œ(ë””ë ‰í† ë¦¬)] [ë…¸ë“œëª…]
  [zk: localhost:2181(CONNECTED) 0] create /pilot-pjt bigdata
  [zk: localhost:2181(CONNECTED) 1] ls /
  [zk: localhost:2181(CONNECTED) 2] get /pilot-pjt
  [zk: localhost:2181(CONNECTED) 3] delete /pilot-pjt
  [zk: localhost:2181(CONNECTED) 4] quit
  ```

## ë¡œê·¸ ì‹œë®¬ë ˆì´í„° ì„¤ì¹˜

- `node2` ì„œë²„ì— `root`ë¡œ ssh ì ‘ì†
- ì‚¬ì „ ì¤€ë¹„ì‚¬í•­

  ```bash
  $> cd
  $> mkdir -p /home/pilot-pjt/working/car-batch-log
  $> mkdir /home/pilot-pjt/working/driver-realtime-log
  $> chmod 777 -R /home/pilot-pjt

  $> rm /usr/bin/java
  rm: cannot remove '/usr/bin/java': ê·¸ëŸ° íŒŒì¼ì´ë‚˜ ë””ë ‰í„°ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤
  $> rm /usr/bin/javac
  rm: cannot remove '/usr/bin/javac': ê·¸ëŸ° íŒŒì¼ì´ë‚˜ ë””ë ‰í„°ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤

  $> ln -s /usr/java/jdk1.7.0_67-cloudera/bin/java /usr/bin/java
  $> ln -s /usr/java/jdk1.7.0_67-cloudera/bin/javac /usr/bin/javac

  $> java -version
  java version "1.7.0_67"
  Java(TM) SE Runtime Environment (build 1.7.0_67-b01)
  Java HotSpot(TM) 64-Bit Server VM (build 24.65-b04, mixed mode)

  $> cp /root/git/bigdata/CH02/bigdata.smartcar.loggen-1.0.jar /home/pilot-pjt/working/
  ```

- ì²«ë²ˆì§¸ ë¡œê·¸ ì‹œë®¬ë ˆì´í„° (**ìš´í–‰ ì •ë³´**)
  ```bash
  $> cd /home/pilot-pjt/working/
  $> java -cp bigdata.smartcar.loggen-1.0.jar com.wikibook.bigdata.smartcar.loggen.DriverLogMain 20190101 10
  ```
- ë¡œê·¸ í™•ì¸ (**í„°ë¯¸ë„ í•˜ë‚˜ ë” ë„ì›Œì„œ** `node2` ì„œë²„ì— `root`ë¡œ ssh ì ‘ì†)

  ```bash
  $> tail -f /home/pilot-pjt//working/driver-realtime-log/SmartCarDriverInfo.log
  ```

- ë‘ë²ˆì§¸ ë¡œê·¸ ì‹œë®¬ë ˆì´í„° (**ìŠ¤ë§ˆíŠ¸ì¹´ ì •ë³´**)

  ```bash
  $> java -cp bigdata.smartcar.loggen-1.0.jar com.wikibook.bigdata.smartcar.loggen.CarLogMain 20190101 10
  ```

- ë¡œê·¸ í™•ì¸ (**í„°ë¯¸ë„ì„ í•˜ë‚˜ ë” ë„ì›Œì„œ** `node2` ì„œë²„ì— `root`ë¡œ ssh ì ‘ì†)

  ```bash
  $> tail -f /home/pilot-pjt//working/SmartCar/SmartCarStatusInfo_20190101.txt
  ```

- í”„ë¡œì„¸ìŠ¤ ì¢…ë£ŒëŠ” `Ctrl + C`

> ë¡œê·¸ì‹œë®¬ë ˆì´í„° ì‹¤í–‰ ì‹œ **ì²«ë²ˆì§¸ ë§¤ê°œë³€ìˆ˜ëŠ” ì‹¤í–‰ë‚ ì§œ**ì´ê³ , **ë‘ë²ˆì§¸ ë§¤ê°œë³€ìˆ˜ëŠ” ìŠ¤ë§ˆíŠ¸ì¹´ ëŒ€ìˆ˜**  
> PCì„±ëŠ¥ì— ë§ê²Œ ë³€ê²½í•´ë„ ë¨

# 03. ë¹…ë°ì´í„° ìˆ˜ì§‘

## Flume Service ì¶”ê°€

- flume Heap Memory ì„¤ì •
  1. `CMí™ˆ > Flume > êµ¬ì„±`
  2. ê²€ìƒ‰ì°½ì— `java heap`ì´ë¼ê³  ê²€ìƒ‰
  3. Agentì˜ Java í™ í¬ê¸°ë¥¼ 100MBë¡œ ì„¤ì • (ê¸°ë³¸ ì„¤ì • : 50 MB)

## Kafka Service ì¶”ê°€

- Parcel ì„¤ì •
  1. `CMí™ˆ > í˜¸ìŠ¤íŠ¸ > Parcel`
  2. ìš°ì¸¡ ìƒë‹¨ `êµ¬ì„±` ë²„íŠ¼ í´ë¦­
  3. ì›ê²© Parcel ë¦¬í¬ì§€í† ë¦¬ URL ê°’ ë³€ê²½ (http://archive.cloudera.com/kafka/parcels/2.2.0/)
  4. `ë‹¤ìš´ë¡œë“œ, ë°°í¬, í™œì„±í™”` ë²„íŠ¼ í´ë¦­
- Kafka ì„œë¹„ìŠ¤ ì¶”ê°€ -> ì‹¤í–‰ ë‹¨ê³„ì—ì„œ ì—ëŸ¬ë°œìƒ
- `Kafka > êµ¬ì„±` ì—ì„œ `Java Heap Size of Broker` ê°’ 256MBë¡œ ì„¤ì •
- ì„œë¹„ìŠ¤ ì¶”ê°€ Resume

> ### ğŸ™‰ kafka ì¬ì‹œì‘í•´ë„ ì˜ ì•ˆë  ê²½ìš° ë¡œê·¸ í™•ì¸ í•„ìš”
>
> ì•„ë˜ ì—ëŸ¬ëŠ” `rm /var/local/kafka/data/meta.properties` ë¥¼ í†µí•´ í•´ê²°
>
> ```
> $> tail /var/log/kafka/kafka-broker-node2.log
> ...
> 2019-06-07 14:42:51,360 FATAL kafka.server.KafkaServerStartable: Fatal error during KafkaServerStartable startup. Prepare to shutdown
> kafka.common.InconsistentBrokerIdException: Configured broker.id 62 doesn't match stored broker.id 50 in meta.properties. If you moved your data, make sure your configured broker.id matches. If you intend to create a new broker, you should remove all data in your data directories (log.dirs).
> ```

## ì—­í•  êµ¬ì„±

![ì—­í• êµ¬ì„±](./images/bigdata_practice/2.PNG)

## Flume ì„¤ì •

- Flume > ì¸ìŠ¤í„´ìŠ¤ > Listì—ì„œ Agent Click > êµ¬ì„±  
  (ConfigíŒŒì¼ ë¿ë§Œ ì•„ë‹ˆë¼ Agent ì´ë¦„ì„ ë°˜ë“œì‹œ ì„¤ì •í•œ í›„ ì¬ì‹œì‘ !!)

  | Source                   | Interceptor        | Channel               | Sink                    |
  | ------------------------ | ------------------ | --------------------- | ----------------------- |
  | SmartCarInfo_SpoolSource | filterInterceptor  | SmartCarInfo_Channel  | SmartCarInfo_LoggerSink |
  | DriverCarInfo_TailSource | filterInterceptor2 | DriverCarInfo_Channel | DriverCarInfo_KafkaSink |

  ```bash
  SmartCar_Agent.sources  = SmartCarInfo_SpoolSource DriverCarInfo_TailSource
  SmartCar_Agent.channels = SmartCarInfo_Channel DriverCarInfo_Channel
  SmartCar_Agent.sinks    = SmartCarInfo_LoggerSink DriverCarInfo_KafkaSink
  ```


    SmartCar_Agent.sources.SmartCarInfo_SpoolSource.type = spooldir
    SmartCar_Agent.sources.SmartCarInfo_SpoolSource.spoolDir = /home/pilot-pjt/working/car-batch-log
    SmartCar_Agent.sources.SmartCarInfo_SpoolSource.deletePolicy = immediate
    SmartCar_Agent.sources.SmartCarInfo_SpoolSource.batchSize = 1000

    SmartCar_Agent.sources.SmartCarInfo_SpoolSource.interceptors = filterInterceptor
    SmartCar_Agent.sources.SmartCarInfo_SpoolSource.interceptors.filterInterceptor.type = regex_filter
    SmartCar_Agent.sources.SmartCarInfo_SpoolSource.interceptors.filterInterceptor.regex = ^\\d{14}
    SmartCar_Agent.sources.SmartCarInfo_SpoolSource.interceptors.filterInterceptor.excludeEvents = false

    SmartCar_Agent.channels.SmartCarInfo_Channel.type = memory
    SmartCar_Agent.channels.SmartCarInfo_Channel.capacity  = 100000
    SmartCar_Agent.channels.SmartCarInfo_Channel.transactionCapacity  = 10000


    SmartCar_Agent.sinks.SmartCarInfo_LoggerSink.type = logger

    SmartCar_Agent.sources.SmartCarInfo_SpoolSource.channels = SmartCarInfo_Channel
    SmartCar_Agent.sinks.SmartCarInfo_LoggerSink.channel = SmartCarInfo_Channel


    SmartCar_Agent.sources.DriverCarInfo_TailSource.type = exec
    SmartCar_Agent.sources.DriverCarInfo_TailSource.command = tail -F /home/pilot-pjt/working/driver-realtime-log/SmartCarDriverInfo.log
    SmartCar_Agent.sources.DriverCarInfo_TailSource.restart = true
    SmartCar_Agent.sources.DriverCarInfo_TailSource.batchSize = 1000

    SmartCar_Agent.sources.DriverCarInfo_TailSource.interceptors = filterInterceptor2
    SmartCar_Agent.sources.DriverCarInfo_TailSource.interceptors.filterInterceptor2.type = regex_filter
    SmartCar_Agent.sources.DriverCarInfo_TailSource.interceptors.filterInterceptor2.regex = ^\\d{14}
    SmartCar_Agent.sources.DriverCarInfo_TailSource.interceptors.filterInterceptor2.excludeEvents = false

    SmartCar_Agent.sinks.DriverCarInfo_KafkaSink.type = org.apache.flume.sink.kafka.KafkaSink
    SmartCar_Agent.sinks.DriverCarInfo_KafkaSink.topic = SmartCar-Topic
    SmartCar_Agent.sinks.DriverCarInfo_KafkaSink.brokerList = node1:9092 node2:9092 node3:9092
    SmartCar_Agent.sinks.DriverCarInfo_KafkaSink.requiredAcks = 1
    SmartCar_Agent.sinks.DriverCarInfo_KafkaSink.batchSize = 1000

    SmartCar_Agent.channels.DriverCarInfo_Channel.type = memory
    SmartCar_Agent.channels.DriverCarInfo_Channel.capacity= 100000
    SmartCar_Agent.channels.DriverCarInfo_Channel.transactionCapacity = 10000

    SmartCar_Agent.sources.DriverCarInfo_TailSource.channels = DriverCarInfo_Channel
    SmartCar_Agent.sinks.DriverCarInfo_KafkaSink.channel = DriverCarInfo_Channel
    ```

## Kafka í…ŒìŠ¤íŠ¸

- í† í”½ ìƒì„±
  ```bash
  kafka-topics --create --zookeeper node1:2181 node2:2181 node3:2181 --replication-factor 3 --partitions 1 --topic SmartCar-Topic
  ```
- í† í”½ í™•ì¸

  ```bash
  kafka-topics --list --zookeeper node1:2181 node2:2181 node3:2181

  kafka-topics --describe --zookeeper node1:2181 node2:2181 node3:2181 --topic SmartCar-Topic
  ```

- í† í”½ ì‚­ì œ
  ```bash
  kafka-topics --delete --zookeeper node1:2181 node2:2181 node3:2181 --topic SmartCar-Topic
  ```
- Producer
  ```bash
  kafka-console-producer --broker-list node1:9092 node2:9092 node3:9092 -topic SmartCar-Topic
  ```
- Consumer

  ````bash
  kafka-console-consumer --zookeeper node1:2181 node2:2181 node3:2181 --topic SmartCar-Topic --from-beginning

      kafka-console-consumer --bootstrap-server node1:9092 node2:9092 node3:9092 --topic SmartCar-Topic --from-beginning
      ```

  > ### ğŸ™‰ Topic ì•ˆì§€ì›Œì§ˆ ë•Œ (ê³„ì† `SmartCar-Topic - marked for deletion` ì´ë¼ê³  ë‚˜ì˜¤ê³  ì •ì‘ ì§€ì›Œì§€ì§€ëŠ” ì•ŠìŒ)
  >
  > ```bash
  > $> zookeeper-client
  > [zk: localhost:2181(CONNECTED) 0] ls /brokers/topics
  > [SmartCar-Topic, SmartCar-Topic1, __consumer_offsets]
  > [zk: localhost:2181(CONNECTED) 1] rmr /brokers/topics/SmartCar-Topic
  > ```
  ````

## ìˆ˜ì§‘ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸

- ë¡œê·¸ ì‹œë®¬ë ˆì´í„° ì‹¤í–‰
  ```bash
  $> cd /home/pilot-pjt/working/
  # ì£¼í–‰ ì •ë³´ (SmartCarDriverInfo.log ìƒì„±)
  $> java -cp bigdata.smartcar.loggen-1.0.jar com.wikibook.bigdata.smartcar.loggen.DriverLogMain 20190101 10
  # ìŠ¤ë§ˆíŠ¸ì¹´ ì •ë³´
  $> java -cp bigdata.smartcar.loggen-1.0.jar com.wikibook.bigdata.smartcar.loggen.CarLogMain 20190101 10
  ```
- íŒŒì¼ ë°€ì–´ë„£ê¸°
  ```bash
  $> mv /home/pilot-pjt/working/SmartCar/SmartCarStatusInfo_20190101.txt /home/pilot-pjt/working/car-batch-log/
  ```
- Flume ì¶œë ¥ ë¡œê·¸ í™•ì¸

  ```bash
  tail -f /var/log/flume-ng/flume-cmf-flume-AGENT-node2.log
  ```

- Kafka Consumer ì‘ë™
  ```bash
  kafka-console-consumer --zookeeper node1:2181 node2:2181 node3:2181 --topic SmartCar-Topic
  ```

# 04 ë¹…ë°ì´í„° ì ì¬ 1

## Flume Agent ìˆ˜ì •

- HDFSì— ì ì¬í•˜ëŠ” Sink êµ¬ì¡°ë¡œ ë³€ê²½

  | Source                   | Interceptor                                                             | Channel               | Sink                                                 |
  | ------------------------ | ----------------------------------------------------------------------- | --------------------- | ---------------------------------------------------- |
  | SmartCarInfo_SpoolSource | timeInterceptor typeInterceptor collectDayInterceptor filterInterceptor | SmartCarInfo_Channel  | <span style="color:red">SmartCarInfo_HdfsSink</span> |
  | DriverCarInfo_TailSource | filterInterceptor2                                                      | DriverCarInfo_Channel | DriverCarInfo_KafkaSink                              |

  ```bash
  SmartCar_Agent.sources  = SmartCarInfo_SpoolSource DriverCarInfo_TailSource
  SmartCar_Agent.channels = SmartCarInfo_Channel DriverCarInfo_Channel
  SmartCar_Agent.sinks    = SmartCarInfo_HdfsSink DriverCarInfo_KafkaSink
  ```


    SmartCar_Agent.sources.SmartCarInfo_SpoolSource.type = spooldir
    SmartCar_Agent.sources.SmartCarInfo_SpoolSource.spoolDir = /home/pilot-pjt/working/car-batch-log
    SmartCar_Agent.sources.SmartCarInfo_SpoolSource.deletePolicy = immediate
    SmartCar_Agent.sources.SmartCarInfo_SpoolSource.batchSize = 1000

    SmartCar_Agent.sources.SmartCarInfo_SpoolSource.interceptors = timeInterceptor typeInterceptor collectDayInterceptor filterInterceptor

    SmartCar_Agent.sources.SmartCarInfo_SpoolSource.interceptors.timeInterceptor.type = timestamp
    SmartCar_Agent.sources.SmartCarInfo_SpoolSource.interceptors.timeInterceptor.preserveExisting = true

    SmartCar_Agent.sources.SmartCarInfo_SpoolSource.interceptors.typeInterceptor.type = static
    SmartCar_Agent.sources.SmartCarInfo_SpoolSource.interceptors.typeInterceptor.key = logType
    SmartCar_Agent.sources.SmartCarInfo_SpoolSource.interceptors.typeInterceptor.value = car-batch-log

    SmartCar_Agent.sources.SmartCarInfo_SpoolSource.interceptors.collectDayInterceptor.type = com.wikibook.bigdata.smartcar.flume.CollectDayInterceptor$Builder

    SmartCar_Agent.sources.SmartCarInfo_SpoolSource.interceptors.filterInterceptor.type = regex_filter
    SmartCar_Agent.sources.SmartCarInfo_SpoolSource.interceptors.filterInterceptor.regex = ^\\d{14}
    SmartCar_Agent.sources.SmartCarInfo_SpoolSource.interceptors.filterInterceptor.excludeEvents = false

    SmartCar_Agent.channels.SmartCarInfo_Channel.type = memory
    SmartCar_Agent.channels.SmartCarInfo_Channel.capacity  = 100000
    SmartCar_Agent.channels.SmartCarInfo_Channel.transactionCapacity  = 10000

    SmartCar_Agent.sinks.SmartCarInfo_HdfsSink.type = hdfs
    SmartCar_Agent.sinks.SmartCarInfo_HdfsSink.hdfs.path = /pilot-pjt/collect/%{logType}/wrk_date=%Y%m%d
    SmartCar_Agent.sinks.SmartCarInfo_HdfsSink.hdfs.filePrefix = %{logType}
    SmartCar_Agent.sinks.SmartCarInfo_HdfsSink.hdfs.fileSuffix = .log
    SmartCar_Agent.sinks.SmartCarInfo_HdfsSink.hdfs.fileType = DataStream
    SmartCar_Agent.sinks.SmartCarInfo_HdfsSink.hdfs.writeFormat = Text
    SmartCar_Agent.sinks.SmartCarInfo_HdfsSink.hdfs.batchSize = 10000
    SmartCar_Agent.sinks.SmartCarInfo_HdfsSink.hdfs.rollInterval = 0
    SmartCar_Agent.sinks.SmartCarInfo_HdfsSink.hdfs.rollCount = 0
    SmartCar_Agent.sinks.SmartCarInfo_HdfsSink.hdfs.idleTimeout = 100
    SmartCar_Agent.sinks.SmartCarInfo_HdfsSink.hdfs.callTimeout = 600000
    SmartCar_Agent.sinks.SmartCarInfo_HdfsSink.hdfs.rollSize = 67108864
    SmartCar_Agent.sinks.SmartCarInfo_HdfsSink.hdfs.threadsPoolSize = 10

    SmartCar_Agent.sources.SmartCarInfo_SpoolSource.channels = SmartCarInfo_Channel
    SmartCar_Agent.sinks.SmartCarInfo_HdfsSink.channel = SmartCarInfo_Channel


    SmartCar_Agent.sources.DriverCarInfo_TailSource.type = exec
    SmartCar_Agent.sources.DriverCarInfo_TailSource.command = tail -F /home/pilot-pjt/working/driver-realtime-log/SmartCarDriverInfo.log
    SmartCar_Agent.sources.DriverCarInfo_TailSource.restart = true
    SmartCar_Agent.sources.DriverCarInfo_TailSource.batchSize = 1000

    SmartCar_Agent.sources.DriverCarInfo_TailSource.interceptors = filterInterceptor2
    SmartCar_Agent.sources.DriverCarInfo_TailSource.interceptors.filterInterceptor2.type = regex_filter
    SmartCar_Agent.sources.DriverCarInfo_TailSource.interceptors.filterInterceptor2.regex = ^\\d{14}
    SmartCar_Agent.sources.DriverCarInfo_TailSource.interceptors.filterInterceptor2.excludeEvents = false

    SmartCar_Agent.sinks.DriverCarInfo_KafkaSink.type = org.apache.flume.sink.kafka.KafkaSink
    SmartCar_Agent.sinks.DriverCarInfo_KafkaSink.topic = SmartCar-Topic
    SmartCar_Agent.sinks.DriverCarInfo_KafkaSink.brokerList = node1:9092 node2:9092 node3:9092
    SmartCar_Agent.sinks.DriverCarInfo_KafkaSink.requiredAcks = 1
    SmartCar_Agent.sinks.DriverCarInfo_KafkaSink.batchSize = 1000

    SmartCar_Agent.channels.DriverCarInfo_Channel.type = memory
    SmartCar_Agent.channels.DriverCarInfo_Channel.capacity= 100000
    SmartCar_Agent.channels.DriverCarInfo_Channel.transactionCapacity = 10000

    SmartCar_Agent.sources.DriverCarInfo_TailSource.channels = DriverCarInfo_Channel
    SmartCar_Agent.sinks.DriverCarInfo_KafkaSink.channel = DriverCarInfo_Channel
    ```

- ì‚¬ìš©ì ì •ì˜ Interceptor ì¶”ê°€
  ```bash
  $> cp /root/git/bigdata/CH04/bigdata.smartcar.flume-1.0.jar /opt/cloudera/parcels/CDH/lib/flume-ng/lib/.
  ```

## ì ì¬ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸

- HDFS pilot-pjt í´ë” ìƒì„±
  ```bash
  $> sudo -u hdfs hdfs dfs -mkdir /pilot-pjt
  $> sudo -u hdfs hdfs dfs -chown flume /pilot-pjt
  ```
- ë¡œê·¸ ì‹œë®¬ë ˆì´í„° ì‹¤í–‰
  ```bash
  $> cd /home/pilot-pjt/working
  $> java -cp bigdata.smartcar.loggen-1.0.jar com.wikibook.bigdata.smartcar.loggen.CarLogMain 20190101 100 &
  ```
- ì •ìƒ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸
  (ì‹œê°„ì´ ê½¤ ê±¸ë¦¼;; íŒŒì¼ì´ ë‹¤ ë§Œë“¤ì–´ì§ˆ ë•Œê¹Œì§€ ê¸°ë‹¤ë ¤ì•¼ í•¨)
  ```bash
  $> ll /home/pilot-pjt/working/SmartCar
  $> tail -f /home/pilot-pjt/working/SmartCar/SmartCarStatusInfo_20190101.txt
  ```
- íŒŒì¼ ë°€ì–´ë„£ê¸°

  ```bash
  $> mv /home/pilot-pjt/working/SmartCar/SmartCarStatusInfo_20190101.txt /home/pilot-pjt/working/car-batch-log/
  ```

- flume ë¡œê·¸ í™•ì¸

  ```bash
  $> tail -f /var/log/flume-ng/flume-cmf-flume-AGENT-node2.log
  ```

- HDFS í™•ì¸

  ```bash
  $> hdfs dfs -ls -R /pilot-pjt/collect/car-batch-log
  drwxr-xr-x   - flume supergroup          0 2019-06-07 16:35 /pilot-pjt/collect/car-batch-log/wrk_date=20190607
  -rw-r--r--   3 flume supergroup   68303308 2019-06-07 16:33 /pilot-pjt/collect/car-batch-log/wrk_date=20190607/car-batch-log.1559892752340.log
  -rw-r--r--   3 flume supergroup   55209183 2019-06-07 16:35 /pilot-pjt/collect/car-batch-log/wrk_date=20190607/car-batch-log.1559892752341.log
  $> hdfs dfs -tail /pilot-pjt/collect/car-batch-log/wrk_date=20190607/car-batch-log.1559892752341.log
  ```

- ë¡œê·¸ ì‹œë®¬ë ˆì´í„° ì¢…ë£Œ
  `bash $> ps -ef | grep smartcar.log root 4362 633 3 10:36 pts/0 00:00:54 java -cp bigdata.smartcar.loggen-1.0.jar com.wikibook.bigdata.smartcar.loggen.CarLogMain 20160101 100 $> kill -9 [pid]`
  > flume spoolingì´ ëë‚˜ë©´ íŒŒì¼ì„ ì‚­ì œí•¨

# 05. ë¹…ë°ì´í„° ì ì¬ 2

## HBase ì„¤ì¹˜

- CMì—ì„œ HBase ì„œë¹„ìŠ¤ ì¶”ê°€
  ![](./images/bigdata_practice/3.PNG) > stale deploy ê¼­ í•´ì•¼ í•¨

- HBase ì„¤ì¹˜ í™•ì¸
- node1ì— rootê³„ì •ìœ¼ë¡œ ì ‘ì†
  ```bash
  $> hbase shell
  ```
- í…ŒìŠ¤íŠ¸
  ```bash
  hbase(main):001:0> create 'smartcar_test_table', 'cf'
  hbase(main):002:0> put 'smartcar_test_table', 'row-key1', 'cf:model', 'Z0001'
  hbase(main):003:0> put 'smartcar_test_table', 'row-key1', 'cf:no', '12345'
  hbase(main):004:0> get 'smartcar_test_table', 'row-key1'
  hbase(main):005:0> disable 'smartcar_test_table'
  hbase(main):006:0> drop 'smartcar_test_table'
  hbase(main):007:0> exit
  ```

## Redis ì„¤ì¹˜

- node2ì— rootê³„ì •ìœ¼ë¡œ ì ‘ì†
- ê¸°ë³¸ í™˜ê²½êµ¬ì„±

  ```bash
  yum install -y gcc*
  yum install -y tcl
  ```

- Redis Download & Build

  ```bash
  $> cd /home/pilot-pjt/
  $> wget http://download.redis.io/releases/redis-3.0.7.tar.gz
  $> tar -xvf redis-3.0.7.tar.gz
  $> cd /home/pilot-pjt/redis-3.0.7
  $> make
  $> make install
  $> cd utils
  $> chmod 755 install_server.sh
  # install_server.sh ì‹¤í–‰ í›„ ì—”í„°ë¥¼ ê³„ì† ì¹˜ë©´ ë¨
  $> ./install_server.sh
  $> service redis_6379 status
  Redis is running (29671)
  ```

- Redis ë°ì´í„° ì €ì¥ ë° ì¡°íšŒí•˜ê¸°
  ```bash
  $> redis-cli
  $> 127.0.0.1:6379> ping
  PONG
  $> 127.0.0.1:6379> set key:1 Hello!Bigdata
  OK
  $> 127.0.0.1:6379> get key:1
  "Hello!Bigdata"
  $> 127.0.0.1:6379> del key:1
  (integer) 1
  $> 127.0.0.1:6379> quit
  ```

## ìŠ¤í†° ì„¤ì¹˜

- íŒŒì¼ ë‹¤ìš´ë¡œë“œ
  ```bash
  cd /home/pilot-pjt/
  wget http://archive.apache.org/dist/storm/apache-storm-0.9.6/apache-storm-0.9.6.tar.gz
  tar -xvf apache-storm-0.9.6.tar.gz
  ln -s apache-storm-0.9.6 storm
  ```
- í™˜ê²½ì„¤ì • íŒŒì¼ ìˆ˜ì •  
  (**ë„ì–´ì“°ê¸°ì— ë§¤ìš°ë§¤ìš° ì£¼ì˜!!!**)

  ```bash
  vi /home/pilot-pjt/storm/conf/storm.yaml
  ########## storm.yaml ì‹œì‘ ##########
  storm.zookeeper.servers:
   - "node1"
   - "node2"
   - "node3"

  storm.local.dir: "/home/pilot-pjt/storm/data"

  nimbus.host: "node2"

  supervisor.slots.ports:
   - 6700
   - 6701
   - 6702

  ui.port: 8087
  ########## storm.yaml ë ##########
  ```

- ë¡œê·¸ë ˆë²¨ ì¡°ì •  
  (ë¡œê·¸ë ˆë²¨ì„ ERRORë¡œ í•˜ë©´ ë¡œê·¸ê°€ ì•ˆì°í˜;;)

  ```bash
  cd /home/pilot-pjt/storm/logback
  vi cluster.xml
  ########## cluster.xml ì‹œì‘ ##########
  <configuration scan="true" scanPeriod="60 seconds">
  ...

  <root level="ERROR">
      <appender-ref ref="A1"/>
  </root>

  <logger name="backtype.storm.security.auth.authorizer" additivity="false">
      <level value="ERROR" />
      <appender-ref ref="ACCESS" />
  </logger>

  <logger name="backtype.storm.metric.LoggingMetricsConsumer" additivity="false" >
      <level value="ERROR"/>
      <appender-ref ref="METRICS"/>
  </logger>

  </configuration>
  ########## cluster.xml ë ##########
  ```

- .bash_profileì— ìŠ¤í†° Path ì„¤ì •

  ```bash
  vi /root/.bash_profile
  ########## .bash_profile ì‹œì‘ ##########
  ...
  PATH=$PATH:$HOME/bin
  PATH=$PATH:/home/pilot-pjt/storm/bin

  export PATH
  ########## .bash_profile ë ##########

  source /root/.bash_profile
  echo $PATH
  ```

- java ë‹¤ì‹œ ì„¤ì •  
  (1.5 versionìœ¼ë¡œ ë°”ê»´ìˆì—ˆìŒ)
  ```bash
  rm /usr/bin/java
  rm /usr/bin/javac
  ln -s /usr/java/jdk1.7.0_67-cloudera/bin/javac /usr/bin/javac
  ln -s /usr/java/jdk1.7.0_67-cloudera/bin/java /usr/bin/java
  ```
- ìŠ¤í†° ì„œë¹„ìŠ¤ ë“±ë¡ì„ ìœ„í•œ ìŠ¤í¬ë¦½íŠ¸ ì„¤ì •

  ```bash
  cp -rf /root/git/bigdata/CH05/ì˜ˆì œ5.1/* /etc/rc.d/init.d/.

  chmod 755 /etc/rc.d/init.d/storm-*
  ```

- ì„œë¹„ìŠ¤ ë“±ë¡ ìŠ¤í¬ë¦½íŠ¸ ê´€ë ¨ Log ë° pid ë””ë ‰í† ë¦¬ ì„¤ì •
  ```bash
  mkdir /var/log/storm
  mkdir /var/run/storm
  ```
- ì„œë¹„ìŠ¤ ì‹œì‘ ë° chkconfig ë“±ë¡

  ```bash
  service storm-nimbus start
  service storm-supervisor start
  service storm-ui start

  chkconfig storm-nimbus on
  chkconfig storm-supervisor on
  chkconfig storm-ui on

  service storm-nimbus status
  service storm-supervisor status
  service storm-ui status
  ```

- storm ui í™•ì¸ (http://node2:8087/)

## Stormì„ í™œìš©í•œ ì‹¤ì‹œê°„ ì ì¬ ê¸°ëŠ¥ êµ¬í˜„

- [github](https://github.com/wikibook/bigdata)ì—ì„œ workspace í´ë” ë°‘ì— ìˆëŠ” `bigdata.smartcar.storm` í”„ë¡œì íŠ¸ ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ ë° ë¹Œë“œ
- **ì„œë²„ ì •ë³´ì™€ Topicì´ í•˜ë“œì½”ë”© ë˜ì–´ ìˆì–´ì„œ ì¼ë¶€ ìˆ˜ì •ì´ í•„ìš”í•¨**
- eclipse ì—ì„œ í”„ë¡œì íŠ¸ Import í•˜ë©´ ì—ëŸ¬ê°€ ë‚˜ëŠ” íŒŒì¼ë“¤ì´ ìˆëŠ”ë° `@Override` ì–´ë…¸í…Œì´ì…˜ ì‚­ì œí–ˆìŒ
- ì•„ë˜ ê¸°ëŠ¥ë“¤ì´ êµ¬í˜„ë˜ì–´ ìˆìŒ
  - Kafka Spout
  - Split Bolt
  - HBase Bolt
  - ì—ìŠ¤í¼ Bolt
  - ë ˆë””ìŠ¤ Bolt

## Redis Client ì–´í”Œë¦¬ì¼€ì´ì…˜ êµ¬í˜„

- [github](https://github.com/wikibook/bigdata)ì—ì„œ workspace í´ë” ë°‘ì— ìˆëŠ” `bigdata.smartcar.redis` í”„ë¡œì íŠ¸ ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ ë° ì‹¤í–‰
- **ì„œë²„ ì •ë³´ê°€ í•˜ë“œì½”ë”© ë˜ì–´ ìˆì–´ì„œ ì¼ë¶€ ìˆ˜ì •ì´ í•„ìš”í•¨**

## HBase í…Œì´ë¸” ìƒì„±

- RegionSplitterë¥¼ ì‚¬ìš©í•˜ì—¬ DriverCarInfo í…Œì´ë¸” ìƒì„±
- column familyëŠ” "cf1"ì„ ì‚¬ìš©í•˜ê³  3ê°œì˜ Region ì„¤ì •
- Regionì— ì ‘ê·¼í•˜ëŠ” ë°©ì‹ì€ HexString

  ````bash
  \$> hbase org.apache.hadoop.hbase.util.RegionSplitter DriverCarInfo HexStringSplit -c 3 -f cf1

      19/06/05 16:45:02 INFO zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
      19/06/05 16:45:02 INFO zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
      19/06/05 16:45:02 INFO zookeeper.ZooKeeper: Client environment:os.name=Linux
      19/06/05 16:45:02 INFO zookeeper.ZooKeeper: Client environment:os.arch=amd64
      19/06/05 16:45:02 INFO zookeeper.ZooKeeper: Client environment:os.version=2.6.32-754.14.2.el6.x86_64
      19/06/05 16:45:02 INFO zookeeper.ZooKeeper: Client environment:user.name=root
      19/06/05 16:45:02 INFO zookeeper.ZooKeeper: Client environment:user.home=/root
      19/06/05 16:45:02 INFO zookeeper.ZooKeeper: Client environment:user.dir=/root
      19/06/05 16:45:02 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=node3:2181,node2:2181,node1:2181 sessionTimeout=60000 watcher=hconnection-0xa29626e0x0, quorum=node3:2181,node2:2181,node1:2181, baseZNode=/hbase
      19/06/05 16:45:02 INFO zookeeper.ClientCnxn: Opening socket connection to server node1/10.146.0.58:2181. Will not attempt to authenticate using SASL (unknown error)
      19/06/05 16:45:02 INFO zookeeper.ClientCnxn: Socket connection established, initiating session, client: /10.146.0.58:52026, server: node1/10.146.0.58:2181
      19/06/05 16:45:02 INFO zookeeper.ClientCnxn: Session establishment complete on server node1/10.146.0.58:2181, sessionid = 0x36b26514f7400ae, negotiated timeout = 60000
      19/06/05 16:45:07 INFO client.HBaseAdmin: Created DriverCarInfo
      19/06/05 16:45:07 INFO client.ConnectionManager$HConnectionImplementation: Closing master protocol: MasterService
      19/06/05 16:45:07 INFO client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x36b26514f7400ae
      19/06/05 16:45:07 INFO zookeeper.ZooKeeper: Session: 0x36b26514f7400ae closed
      19/06/05 16:45:07 INFO zookeeper.ClientCnxn: EventThread shut down
      ```

  > region ìˆ˜ë¥¼ 3ìœ¼ë¡œ í•˜ë©´ Region Serverì— Requestê°€ ì œëŒ€ë¡œ ë¶„ë°°ê°€ ì•ˆë¨ (10ìœ¼ë¡œ ëŠ˜ë ¤ë³´ë‹ˆ ë¶„ë°°ê°€ ì˜ë˜ì—ˆìŒ ^^)
  ````

## ìŠ¤í†° Topology ë°°í¬

- íŒ¨í‚¤ì§• ëœ jar íŒŒì¼ ì—…ë¡œë“œ (íŒŒì¼ì§ˆë¼ë¡œ ì—…ë¡œë“œ í•´ë„ ì¢‹ì„ë“¯)
  ```bash
  $> scp git/bigdata/workspace/bigdata.smartcar.storm/target/bigdata.smartcar.storm-1.0.jar root@node2:/home/pilot-pjt/working/
  ```
- ìŠ¤í†° topology ë°°í¬

  ```bash
  $> cd /home/pilot-pjt/working
  # bigdata.smartcar.storm-1.0.jar ì— í¬í•¨ëœ SmartCarDriverTopology íŒŒì¼ì„ DriverCarInfoë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ë°°í¬
  $> storm jar bigdata.smartcar.storm-1.0.jar com.wikibook.bigdata.smartcar.storm.SmartCarDriverTopology DriverCarInfo

  788  [main] INFO  backtype.storm.StormSubmitter - Jar not uploaded to master yet. Submitting jar...
  802  [main] INFO  backtype.storm.StormSubmitter - Uploading topology jar bigdata.smartcar.storm-1.0.jar to assigned location: /home/pilot-pjt/storm/data/nimbus/inbox/stormjar-2e5ab9b2-8a02-495a-a9bb-07a874e5807e.jar
  1142 [main] INFO  backtype.storm.StormSubmitter - Successfully uploaded topology jar to assigned location: /home/pilot-pjt/storm/data/nimbus/inbox/stormjar-2e5ab9b2-8a02-495a-a9bb-07a874e5807e.jar
  1142 [main] INFO  backtype.storm.StormSubmitter - Submitting topology DriverCarInfo in distributed mode with conf {"nimbus.host":"node2","HBASE_CONFIG":{"hbase.rootdir":"hdfs:\/\/node1:8020\/hbase"},"storm.zookeeper.port":2181,"topology.debug":true,"nimbus.thrift.port":6627,"storm.zookeeper.servers":["node2"]}
  1571 [main] INFO  backtype.storm.StormSubmitter - Finished submitting topology: DriverCarInfo
  ```

- ìŠ¤í†° UI í™”ë©´ì„ í†µí•´ í™•ì¸ (http://node2:8087/)  
  Topology summaryì— DriverCarInfo ê°€ í™œì„±í™” ë˜ì–´ ìˆìŒ

- ìŠ¤í†° Topology ì œê±° ëª…ë ¹ì–´ (**í•„ìš”í•  ë•Œ ì‹¤í–‰**)
  ```bash
  $> storm kill [Topology ëª…]
  ```

## ì‹¤ì‹œê°„ ì ì¬ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸

- ë¡œê·¸ ì‹œë®¬ë ˆì´í„° ì‹¤í–‰

  ```bash
  $> java -cp /home/pilot-pjt/working/bigdata.smartcar.loggen-1.0.jar com.wikibook.bigdata.smartcar.loggen.DriverLogMain 20190101 20 &

  # Process í™•ì¸
  $> ps -ef | grep DriverLogMain
  ```

- ì •ìƒ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸
  ```bash
  $> ll /home/pilot-pjt/working/driver-realtime-log
  $> tail -f /home/pilot-pjt/working/driver-realtime-log/SmartCarDriverInfo.log
  ```
- Kafka consumer

  ```bash
  kafka-console-consumer --bootstrap-server node1:9092 node2:9092 node3:9092 --topic SmartCar-Topic
  ```

- HBaseì— ì ì¬ ë°ì´í„° í™•ì¸

  ```bash
  $> hbase shell

  hbase(main):001:0> count 'DriverCarInfo'
  hbase(main):002:0> scan 'DriverCarInfo', {LIMIT=>20}
  ROW                     COLUMN+CELL
  00000010109102-B0004    column=cf1:area_number, timestamp=1559897465201, value=A01
  00000010109102-B0004    column=cf1:break_pedal, timestamp=1559897465201, value=0
  00000010109102-B0004    column=cf1:car_number, timestamp=1559897465201, value=B0004
  00000010109102-B0004    column=cf1:date, timestamp=1559897465201, value=20190101000000
  00000010109102-B0004                                                column=cf1:direct_light, timestamp=1559897465201, value=L
  00000010109102-B0004                                                column=cf1:speed, timestamp=1559897465201, value=10
  00000010109102-B0004                                                column=cf1:speed_pedal, timestamp=1559897465201, value=2
  00000010109102-B0004                                                column=cf1:steer_angle, timestamp=1559897465201, value=L3
  00000010109102-C0019                                                column=cf1:area_number, timestamp=1559897467778, value=A07
  00000010109102-C0019                                                column=cf1:break_pedal, timestamp=1559897467778, value=1
  00000010109102-C0019                                                column=cf1:car_number, timestamp=1559897467778, value=C0019
  00000010109102-C0019                                                column=cf1:date, timestamp=1559897467778, value=20190101000000
  00000010109102-C0019                                                column=cf1:direct_light, timestamp=1559897467778, value=N
  00000010109102-C0019                                                column=cf1:speed, timestamp=1559897467778, value=0
  00000010109102-C0019                                                column=cf1:speed_pedal, timestamp=1559897467778, value=0
  00000010109102-C0019                                                column=cf1:steer_angle, timestamp=1559897467778, value=F

  # ì²«ë²ˆì¨°ë¡œ ì¡°íšŒë˜ëŠ” ROW Keyë¥¼ ì¡°ê±´ì ˆì— ì¶”ê°€í•˜ì—¬ scan
  hbase(main):003:0> scan 'DriverCarInfo', {STARTROW=>'00000010109102-B0004', LIMIT=>1}

  hbase(main):004:0> scan 'DriverCarInfo', {COLUMNS=>['cf1:car_number','cf1:area_number'],FILTER=>"RowFilter(=,'regexstring:10109102') AND SingleColumnValueFilter('cf1','area_number',=,'regexstring:A01')"}
  ```

- Redis Client ì‹¤í–‰

  ```bash
  scp git/bigdata/workspace/bigdata.smartcar.redis/target/bigdata.smartcar.redis-1.0.jar root@node2:/home/pilot-pjt/working/

  java -cp /home/pilot-pjt/working/bigdata.smartcar.redis-1.0.jar com.wikibook.bigdata.smartcar.redis.OverSpeedCarInfo 20190103
  ```

# 06. ë¹…ë°ì´í„° íƒìƒ‰

- hive ì„¤ì¹˜
  ![hive ì„¤ì¹˜](./images/bigdata_practice/4.PNG)
- oozie ì„¤ì¹˜  
  Dependency : Hbase, HDFS, Hive, YARN, ZooKeeper
  ![oozie ì„¤ì¹˜](./images/bigdata_practice/5.PNG)
- hue ì„¤ì¹˜
  ![hue ì„¤ì¹˜](./images/bigdata_practice/6.PNG)
- Spark ì„¤ì¹˜  
  Dependency : Hbase, HDFS, YARN, ZooKeeper
  ![Spark ì„¤ì¹˜](./images/bigdata_practice/7.PNG)
  > Stale Deploy ê¼­ í•´ì•¼í•¨

## Hue

- Hue ì •ë³´ í™•ì¸ (http://node1:8888/about/)
- Hbase ì—ëŸ¬  
  (Failed to authenticate to HBase Thrift Server, check authentication configurations.)

  1. HBase Thrift Server ì¤‘ì§€
  2. `hbase.regionserver.thrift.server.type` ì†ì„± `TNonblockingServer`ë¡œ ë³€ê²½
  3. HBase Thrift Server ì‹œì‘

- Hue Database ì—”ì§„ ê´€ë ¨ ì—ëŸ¬
  1. hue ì„œë¹„ìŠ¤ ì¤‘ì§€
  2. `node1` ì„œë²„ì— `root`ë¡œ ssh ì ‘ì†
  3. mysql ë¡œê·¸ì¸
     ```bash
     $> mysql -u root
     ```
  4. hue ë°ì´í„°ë² ì´ìŠ¤ì˜ ìŠ¤í‚¤ë§ˆ ì •ë³´ í™•ì¸
     ```sql
     SELECT table_schema, table_name, engine
     FROM information_schema.tables
     WHERE engine = 'MyISAM' AND table_schema = 'hue';
     ```
  5. set_engine_innodb.ddl íŒŒì¼ ìƒì„±
     ```bash
     $> mysql -u root -e \
     "SELECT CONCAT('ALTER TABLE ',table_schema,'.',table_name,' engine=InnoDB;') \
     FROM information_schema.tables \
     WHERE engine = 'MyISAM' AND table_schema = 'hue';" \
     | grep "ALTER TABLE hue" > /tmp/set_engine_innodb.ddl
     ```
  6. set_engine_innodb.ddl ì‹¤í–‰
     ```bash
     $> mysql -u root < /tmp/set_engine_innodb.ddl
     ```

### Hive External í…Œì´ë¸” ìƒì„± ë° ì¡°íšŒ

- External í…Œì´ë¸” ìƒì„±
  ```sql
  create external table if not exists SmartCar_Status_Info (
  reg_date string,
  car_number string,
  tire_fl string,
  tire_fr string,
  tire_bl string,
  tire_br string,
  light_fl string,
  light_fr string,
  light_bl string,
  light_br string,
  engine string,
  break string,
  battery string
  )
  partitioned by( wrk_date string )
  row format delimited
  fields terminated by ','
  stored as textfile
  location '/pilot-pjt/collect/car-batch-log/'
  ```
- Partition ì •ë³´ ì¶”ê°€  
  wrk_data ëŠ” ì‘ì—…í–ˆë˜ ì¼ìë¥¼ ë„£ì–´ì•¼ í•¨.(ì˜ ëª¨ë¥´ê² ìœ¼ë©´ `/pilot-pjt/collect/car-batch-log/`ë¡œ ê°€ì„œ í™•ì¸)
  ```sql
  ALTER TABLE smartcar_status_info ADD PARTITION(wrk_date='20190607');
  ```
- 5ê°œ ì¡°íšŒ

  ```sql
  select * from smartcar_status_info limit 5;
  ```

- ì¡°ê±´ ì¡°íšŒ (1m16s ê±¸ë ¸ìŒ;;)
  ```sql
  select car_number, avg(battery) as battery_avg
  from smartcar_status_info
  where battery < 60
  group by car_number;
  ```
  > Job Browserì—ì„œë„ í™•ì¸ ê°€ëŠ¥

### í•˜ì´ë¸Œë¥¼ ì´ìš©í•œ HBase ë°ì´í„° ê²€ìƒ‰

- HBaseStorageHandlerë¥¼ ì´ìš©í•´ External í…Œì´ë¸” ë§Œë“¤ê¸°
  ```sql
  CREATE EXTERNAL TABLE SmartCar_Drive_Info(
  r_key string, s
  date string,
  car_number string,
  speed_pedal string,
  break_pedal string,
  steer_angle string,
  direct_light string,
  speed string,
  area_number string)
  STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
  WITH SERDEPROPERTIES (
  "hbase.columns.mapping" = "cf1:date,cf1:car_number,
                              cf1:speed_pedal,
                              cf1:break_pedal,
                              cf1:steer_angle,
                              cf1:direct_light,
                              cf1:speed,
                              cf1:area_number")
  TBLPROPERTIES(
  "hbase.table.name" = "DriverCarInfo");
  ```
- 10ê±´ ì¡°íšŒ
  ```sql
  select * from smartcar_drive_info limit 10;
  ```

## ë°ì´í„°ì…‹ ì¶”ê°€

> **ìŠ¤ë§ˆíŠ¸ì¹´ ë§ˆìŠ¤í„° ë°ì´í„°**ì™€ **ìŠ¤ë§ˆíŠ¸ì¹´ ì°¨ëŸ‰ìš©í’ˆ êµ¬ë§¤ ì´ë ¥ ë°ì´í„°**ë¥¼ ì¶”ê°€

- í´ë” ìƒì„±

  ```bash
  # car-master í´ë” ìƒì„±
  sudo -u hdfs hdfs dfs -mkdir /pilot-pjt/collect/car-master
  sudo -u hdfs hdfs dfs -chown admin /pilot-pjt/collect/car-master
  # buy-list í´ë” ìƒì„±
  sudo -u hdfs hdfs dfs -mkdir /pilot-pjt/collect/buy-list
  sudo -u hdfs hdfs dfs -chown admin /pilot-pjt/collect/buy-list
  ```

- hue íŒŒì¼ ë¸Œë¼ìš°ì € ì—…ë¡œë“œ ê¸°ëŠ¥ì„ í†µí•´ ë°ì´í„° ì—…ë¡œë“œ
- External í…Œì´ë¸” ìƒì„±

  ```sql
  CREATE EXTERNAL TABLE SmartCar_Master (
  car_number string,
  sex string,
  age string,
  marriage string,
  region string,
  job string,
  car_capacity string,
  car_year string,
  car_model string
  )
  row format delimited
  fields terminated by '|'
  stored as textfile
  location '/pilot-pjt/collect/car-master'
  ```

  ```sql
  CREATE EXTERNAL TABLE SmartCar_Item_BuyList (
  car_number string,
  Item string,
  score string,
  month string
  )
  row format delimited
  fields terminated by ','
  stored as textfile
  location '/pilot-pjt/collect/buy-list'
  ```

- ë°ì´í„° ì¡°íšŒ
  ```sql
  select * from smartcar_master
  ```
  ```sql
  select * from smartcar_item_buylist
  ```

## Spark ë¥¼ ì´ìš©í•œ ë°ì´í„° íƒìƒ‰

- spark-shell ì‹¤í–‰
  ```bash
  $> spark-shell
  ```
- ì—ëŸ¬ ë°œìƒ

  ```
      ____              __
      / __/__  ___ _____/ /__
      _\ \/ _ \/ _ `/ __/  '_/
  /___/ .__/\_,_/_/ /_/\_\   version 1.6.0
      /_/

  Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_67)
  Type in expressions to have them evaluated.
  Type :help for more information.

  19/06/10 11:09:48 ERROR spark.SparkContext: Error initializing SparkContext.
  java.lang.IllegalArgumentException: Required executor memory (1024+384 MB) is above the max threshold (1024 MB) of this cluster! Please check the values of 'yarn.scheduler.maximum-allocation-mb' and/or 'yarn.nodemanager.resource.memory-mb'.
  ```

- spark-shell ì‹¤í–‰ (with executor-memory option)
  ```bash
  spark-shell --executor-memory 512MB
  ```
- ì—ëŸ¬ ë°œìƒ

  ```
      ____              __
      / __/__  ___ _____/ /__
      _\ \/ _ \/ _ `/ __/  '_/
  /___/ .__/\_,_/_/ /_/\_\   version 1.6.0
      /_/

  Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_67)
  Type in expressions to have them evaluated.
  Type :help for more information.
  19/06/10 11:11:01 ERROR spark.SparkContext: Error initializing SparkContext.
  org.apache.hadoop.security.AccessControlException: Permission denied: user=root, access=WRITE, inode="/user":hdfs:supergroup:drwxr-xr-x
  ```

- root ê³„ì •ì˜ home í´ë” ìƒì„±

  ```bash
  sudo -u hdfs hdfs dfs -mkdir /user/root
  sudo -u hdfs hdfs dfs -chown root /user/root
  ```

- spark-shell ì‹¤í–‰ (with executor-memory option)

  ```bash
  spark-shell --executor-memory 512MB
  ```

- ë°ì´í„°ì…‹ íƒìƒ‰

  ```bash
  scala> val smartcar_master_df = sqlContext.sql("SELECT * FROM SmartCar_Master WHERE age >=18")
  scala> smartcar_master_df.show()
  scala> smartcar_master_df.saveAsTable("SmartCar_Master_Over18")

  scala> val smartcar_master_over18_df = sqlContext.sql("select * from smartcar_master_over18 where age > 30 and sex = 'ë‚¨'")
  scala> smartcar_master_over18_df.show()
  ```

- `hue > hive query editor`ì—ì„œ ë™ì¼í•œ ì¿¼ë¦¬ ì‹¤í–‰ í›„ ì‹¤í–‰ì‹œê°„ ë¹„êµ

## í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±

- ìŠ¤ë§ˆíŠ¸ì¹´ ì •ë³´ ìƒì„±

  ```bash
  $> cd /home/pilot-pjt/working/
  # ìŠ¤ë§ˆíŠ¸ì¹´ ì •ë³´
  $> java -cp bigdata.smartcar.loggen-1.0.jar com.wikibook.bigdata.smartcar.loggen.CarLogMain 20190610 100 &
  # í™•ì¸
  $> tail -f /home/pilot-pjt/working/SmartCar/SmartCarStatusInfo_20190610.txt
  # tail í™”ë©´ì—ì„œ ë”ì´ìƒ ë°ì´í„°ê°€ ìƒì„±ë˜ì§€ ì•Šìœ¼ë©´ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
  $> ps -ef | grep smartcar.log
  $> pkill -f smartcar.log

  $> mv /home/pilot-pjt/working/SmartCar/SmartCarStatusInfo_20190610.txt /home/pilot-pjt/working/car-batch-log/
  $> hdfs dfs -ls -R /pilot-pjt/collect/car-batch-log/
  ```

- ìŠ¤ë§ˆíŠ¸ì¹´ ì‹¤ì‹œê°„ ìš´í–‰ ë¡œê·¸
  ```bash
  $> cd /home/pilot-pjt/working/
  # ìŠ¤ë§ˆíŠ¸ì¹´ ì‹¤ì‹œê°„ ìš´í–‰ ì •ë³´
  $> java -cp bigdata.smartcar.loggen-1.0.jar com.wikibook.bigdata.smartcar.loggen.DriverLogMain 20190610 100 &
  # í™•ì¸
  $> tail -f /home/pilot-pjt/working/driver-realtime-log/SmartCarDriverInfo.log
  $> redis-cli
  127.0.0.1:6379> smembers 20190610
  1) "R0039-20190610000150"
  2) "E0064-20190610000140"
  3) "O0076-20190610000240"
  4) "C0023-20190610000304"
  5) "S0025-20190610000156"
  # redisì— ê³¼ì†ì°¨ëŸ‰ì´ 3ê±´ ì´ìƒ ë³´ì´ë©´ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
  $> ps -ef | grep smartcar.log
  $> pkill -f smartcar.log
  ```
  `Hue > Data Browsers > HBase` ë©”ë‰´ì—ì„œ `DriverCarInfo` í…Œì´ë¸”ì„ ì„ íƒí•˜ì—¬ ìš´í–‰ ë°ì´í„°ê°€ ì˜ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸ í•„ìš”  
   (ì‹¤í–‰ì¼ íŒŒë¼ë¯¸í„° ê°’ì„ `20190610` ìœ¼ë¡œ í–ˆìœ¼ë©´ `00000001609102-A0052, 00000001609102-B0006` ê°™ì€ ë¡œìš°í‚¤ê°€ ìƒê¹€)

## OOZIE

### ë°ì´í„° ì£¼ì œì˜ì—­

|                    | Subject1                                                  | Subject2                                                    | Subject3                      | Subject4                              | Subject5                                                           |
| ------------------ | --------------------------------------------------------- | ----------------------------------------------------------- | ----------------------------- | ------------------------------------- | ------------------------------------------------------------------ |
| ì´ìš© í…Œì´ë¸”        | - ìŠ¤ë§ˆíŠ¸ì¹´ ë§ˆìŠ¤í„° ë°ì´í„° <br/>- ìŠ¤ë§ˆíŠ¸ì¹´ ìƒíƒœ ì •ë³´ ë°ì´í„° | - ìŠ¤ë§ˆíŠ¸ì¹´ ë§ˆìŠ¤í„° ë°ì´í„° <br/>- ìŠ¤ë§ˆíŠ¸ì¹´ ìš´ì „ì ìš´í–‰ ë°ì´í„° | ìŠ¤ë§ˆíŠ¸ì¹´ ìš´ì „ì ìš´í–‰ê¸°ë¡ ì •ë³´ | ìŠ¤ë§ˆíŠ¸ì¹´ ìƒíƒœ ëª¨ë‹ˆí„°ë§ ì •ë³´           | - ìŠ¤ë§ˆíŠ¸ì¹´ ë§ˆìŠ¤í„° ë°ì´í„° <br/>- ìŠ¤ë§ˆíŠ¸ì¹´ ì°¨ëŸ‰ìš”í’ˆ êµ¬ë§¤ ì´ë ¥ ë°ì´í„° |
| ìƒì„±í•  Mart í…Œì´ë¸” | Managed_SmartCar_Status_Info                              | Managed_SmartCar_Drive_Info                                 | Managed_SmartCar_Symptom_Info | Managed_SmartCar_Emergency_Check_Info | Managed_SmartCar_Item_BuyList_Info                                 |

### Subject1. ìŠ¤ë§ˆíŠ¸ì¹´ ìƒíƒœ ì •ë³´ ëª¨ë‹ˆí„°ë§ ì›Œí¬í”Œë¡œ ì‘ì„±

#### Workflow

0. ì‘ì—… í´ë” ìƒì„±

   ```bash
   sudo -u hdfs hdfs dfs -mkdir /pilot-pjt/workflow
   sudo -u hdfs hdfs dfs -mkdir /pilot-pjt/workflow/subject1
   sudo -u hdfs hdfs dfs -mkdir /pilot-pjt/workflow/subject2
   sudo -u hdfs hdfs dfs -mkdir /pilot-pjt/workflow/subject3
   sudo -u hdfs hdfs dfs -mkdir /pilot-pjt/workflow/subject4
   sudo -u hdfs hdfs dfs -mkdir /pilot-pjt/workflow/subject5

   sudo -u hdfs hdfs dfs -chown -R admin /pilot-pjt/workflow
   ```

1. `/pilot-pjt/workflow/subject1` ë””ë ‰í† ë¦¬ ë°‘ì— ì•„ë˜ 3ê°œ íŒŒì¼ ì¶”ê°€

   - `create_table_managed_smartcar_status_info.hql`
     ```sql
     create table if not exists Managed_SmartCar_Status_Info (
     car_number string,
     sex string,
     age string,
     marriage string,
     region string,
     job string,
     car_capacity string,
     car_year string,
     car_model string,
     tire_fl string,
     tire_fr string,
     tire_bl string,
     tire_br string,
     light_fl string,
     light_fr string,
     light_bl string,
     light_br string,
     engine string,
     break string,
     battery string,
     reg_date string
     )
     partitioned by( biz_date string )
     row format delimited
     fields terminated by ','
     stored as textfile;
     ```
   - `alter_partition_smartcar_status_info.hql`
     ```sql
     alter table SmartCar_Status_Info add if not exists partition(wrk_date='${working_day}');
     ```
   - `insert_table_managed_smartcar_status_info.hql`

     ```sql
     set hive.exec.dynamic.partition=true;
     set hive.exec.dynamic.partition.mode=nonstrict;

     insert overwrite table Managed_SmartCar_Status_Info partition(biz_date)
     select
     t1.car_number,
     t1.sex,
     t1.age,
     t1.marriage,
     t1.region,
     t1.job,
     t1.car_capacity,
     t1.car_year,
     t1.car_model,
     t2.tire_fl,
     t2.tire_fr,
     t2.tire_bl,
     t2.tire_br,
     t2.light_fl,
     t2.light_fr,
     t2.light_bl,
     t2.light_br,
     t2.engine,
     t2.break,
     t2.battery,
     t2.reg_date,
     substring(t2.reg_date, 0, 8) as biz_date
     from  SmartCar_Master_Over18 t1 join SmartCar_Status_Info t2
     on t1.car_number = t2.car_number and t2.wrk_date = '${working_day}';
     ```

1. Hue > Workflows > í¸ì§‘ê¸° > Workflow ë©”ë‰´ì—ì„œ ìƒì„± ë²„íŠ¼ í´ë¦­
1. ToolBoxì—ì„œ ì‘ì—… ì„ íƒ í›„ Hive Server2ì‘ì—…ì„ ì‘ì—… ë…¸ë“œì— Drag & Drop

   | ìˆœì„œ | íŒŒì¼ëª…                                          | ë§¤ê°œë³€ìˆ˜              |
   | ---- | ----------------------------------------------- | --------------------- |
   | 1    | `create_table_managed_smartcar_status_info.hql` | -                     |
   | 2    | `alter_partition_smartcar_status_info.hql`      | working_day=\${today} |
   | 3    | `insert_table_managed_smartcar_status_info.hql` | working_day=\${today} |

   > ì‘ì—…ì„ ë°”ë¡œ ì‹¤í–‰í•˜ê³  ì‹¶ì„ë• ë§¤ê°œë³€ìˆ˜ë¥¼ ì˜¤ëŠ˜ë‚ ì§œë¡œ ì„¤ì •í•´ì£¼ë©´ ë¨ (`working_day=20190610`)

1. Workflow ì´ë¦„ì„ `Subject1-Workflow`ë¡œ ì„¤ì • í›„ ì €ì¥, ì œì¶œ ë²„íŠ¼ í´ë¦­

#### Coordinator

1. `Hue > Workflows > í¸ì§‘ê¸° > Coordinator` ë©”ë‰´ì—ì„œ ìƒì„± ë²„íŠ¼ í´ë¦­
2. ì´ë¦„ì€ `Subject1-Coordinator` ë¡œ ì„¤ì •
3. ì›Œí¬í”Œë¡œëŠ” ì•ì„œ ì‘ì„±í–ˆë˜ `Subject1-Workflow` ì„ íƒ
4. ì‘ì—… ê°„ê²© ì„¤ì •
   | í•­ëª© | ì„¤ì • ê°’ |
   | -- | -- |
   | ì‹¤í–‰ ê°„ê²© | ë§¤ì¼, 01ì‹œ |
   | ì‹œì‘ ì¼ì | 2019ë…„ 6ì›” 9ì¼ 00ì‹œ 00ë¶„ |
   | ì¢…ë£Œ ì¼ì | 2019ë…„ 6ì›” 17ì¼ 23ì‹œ 59ë¶„ |
   | ì‹œê°„ëŒ€ | Asia/Seoul |
5. ë§¤ê°œë³€ìˆ˜ ì„¤ì • (`today`)
   ```
   ${coord:formatTime(coord:dateTzOffset(coord:nominalTime(), "Asia/Seoul"), 'yyyyMMdd')}
   ```
6. ì €ì¥ ë° ì œì¶œ
7. `Hue > Workflows > ëŒ€ì‹œë³´ë“œ > Coordinator` ì—ì„œ í™•ì¸

8. ì‘ì—…ì´ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ëë‹¤ë©´ Hive Editorì—ì„œ ì•„ë˜ ì¿¼ë¦¬ë¥¼ í†µí•´ í™•ì¸ ê°€ëŠ¥
   ```sql
   select * from managed_smartcar_status_info where biz_date = '20190610' limit 10;
   ```

## Subject2. ìŠ¤ë§ˆíŠ¸ì¹´ ìš´ì „ì ìš´í–‰ ê¸°ë¡ ì •ë³´ ì›Œí¬ë¥¼ë¡œ ì‘ì„±

#### Workflow

1. `/pilot-pjt/collect/drive-log/` ë””ë ‰í† ë¦¬ ìƒì„±

   ```bash
   sudo -u hdfs hdfs dfs -mkdir /pilot-pjt/collect/drive-log/
   sudo -u hdfs hdfs dfs -chown admin /pilot-pjt/collect/drive-log/
   ```

2. `/pilot-pjt/workflow/subject2` ë””ë ‰í† ë¦¬ ë°‘ì— ì•„ë˜ 4ê°œ íŒŒì¼ ì¶”ê°€

   - `create_table_smartcar_drive_info_2.hql`
     ```sql
     create external table if not exists SmartCar_Drive_Info_2 (
         r_key string,
         date string,
         car_number string,
         speed_pedal string,
         break_pedal string,
         steer_angle string,
         direct_light string,
         speed string,
         area_number string
     )
     partitioned by( wrk_date string )
     row format delimited
     fields terminated by ','
     stored as textfile
     location '/pilot-pjt/collect/drive-log/'
     ```
   - `insert_table_smartcar_drive_info_2.hql`

     ```sql
     set hive.exec.dynamic.partition=true;
     set hive.exec.dynamic.partition.mode=nonstrict;

     insert overwrite table SmartCar_Drive_Info_2 partition(wrk_date)
     select
         r_key ,
         date ,
         car_number ,
         speed_pedal ,
         break_pedal ,
         steer_angle ,
         direct_light ,
         speed ,
         area_number ,
         substring(date, 0, 8) as wrk_date
     from SmartCar_Drive_Info
     where substring(date, 0, 8) = '${working_day}';
     ```

   - `create_table_managed_smartcar_drive_info.hql`

     ```sql
     create table if not exists Managed_SmartCar_Drive_Info (
         car_number string,
         sex string,
         age string,
         marriage string,
         region string,
         job string,
         car_capacity string,
         car_year string,
         car_model string,

         speed_pedal string,
         break_pedal string,
         steer_angle string,
         direct_light string,
         speed string,
         area_number string,
         reg_date string
     )
     partitioned by( biz_date string )
     row format delimited
     fields terminated by ','
     stored as textfile;
     ```

   - `insert_table_managed_smartcar_drive_info.hql`

     ```sql
     set hive.exec.dynamic.partition=true;
     set hive.exec.dynamic.partition.mode=nonstrict;

     insert overwrite table Managed_SmartCar_drive_Info partition(biz_date)
     select
         t1.car_number,
         t1.sex,
         t1.age,
         t1.marriage,
         t1.region,
         t1.job,
         t1.car_capacity,
         t1.car_year,
         t1.car_model,
         t2.speed_pedal,
         t2.break_pedal,
         t2.steer_angle,
         t2.direct_light ,
         t2.speed ,
         t2.area_number ,
         t2.date,
         substring(t2.date, 0, 8) as biz_date
     from  SmartCar_Master_Over18 t1 join SmartCar_Drive_Info_2 t2
     on t1.car_number = t2.car_number and substring(t2.date,0,8) = '${working_day}';
     ```

3. Hue > Workflows > í¸ì§‘ê¸° > Workflow ë©”ë‰´ì—ì„œ ìƒì„± ë²„íŠ¼ í´ë¦­
4. ToolBoxì—ì„œ ì‘ì—… ì„ íƒ í›„ Hive Server2ì‘ì—…ì„ ì‘ì—… ë…¸ë“œì— Drag & Drop

   | ìˆœì„œ | íŒŒì¼ëª…                                         | ë§¤ê°œë³€ìˆ˜              |
   | ---- | ---------------------------------------------- | --------------------- |
   | 1    | `create_table_smartcar_drive_info_2.hql`       | -                     |
   | 2    | `insert_table_smartcar_drive_info_2.hql`       | working_day=\${today} |
   | 3    | `create_table_managed_smartcar_drive_info.hql` | -                     |
   | 4    | `insert_table_managed_smartcar_drive_info.hql` | working_day=\${today} |

   > ì‘ì—…ì„ ë°”ë¡œ ì‹¤í–‰í•˜ê³  ì‹¶ì„ë• ë§¤ê°œë³€ìˆ˜ë¥¼ ì˜¤ëŠ˜ë‚ ì§œë¡œ ì„¤ì •í•´ì£¼ë©´ ë¨ (`working_day=20190610`)

5. Workflow ì´ë¦„ì„ `Subject2-Workflow`ë¡œ ì„¤ì • í›„ ì €ì¥, ì œì¶œ ë²„íŠ¼ í´ë¦­

### Coordinator

1. `Hue > Workflows > í¸ì§‘ê¸° > Coordinator` ë©”ë‰´ì—ì„œ ìƒì„± ë²„íŠ¼ í´ë¦­
2. ì´ë¦„ì€ `Subject2-Coordinator` ë¡œ ì„¤ì •
3. ì›Œí¬í”Œë¡œëŠ” ì•ì„œ ì‘ì„±í–ˆë˜ `Subject2-Workflow` ì„ íƒ
4. ì‘ì—… ê°„ê²© ì„¤ì •
   | í•­ëª© | ì„¤ì • ê°’ |
   | -- | -- |
   | ì‹¤í–‰ ê°„ê²© | ë§¤ì¼, 02ì‹œ |
   | ì‹œì‘ ì¼ì | 2019ë…„ 6ì›” 9ì¼ 00ì‹œ 00ë¶„ |
   | ì¢…ë£Œ ì¼ì | 2019ë…„ 6ì›” 17ì¼ 23ì‹œ 59ë¶„ |
   | ì‹œê°„ëŒ€ | Asia/Seoul |
5. ë§¤ê°œë³€ìˆ˜ ì„¤ì • (`today`)
   ```
   ${coord:formatTime(coord:dateTzOffset(coord:nominalTime(), "Asia/Seoul"), 'yyyyMMdd')}
   ```
6. ì €ì¥ ë° ì œì¶œ
7. `Hue > Workflows > ëŒ€ì‹œë³´ë“œ > Coordinator` ì—ì„œ í™•ì¸

8. ì‘ì—…ì´ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ëë‹¤ë©´ Hive Editorì—ì„œ ì•„ë˜ ì¿¼ë¦¬ë¥¼ í†µí•´ í™•ì¸ ê°€ëŠ¥
   ```sql
   select * from managed_smartcar_drive_info where biz_date = '20190610' limit 10;
   ```

## Subject3. ì´ìƒ ìš´ì „ íŒ¨í„´ ìŠ¤ë§ˆíŠ¸ì¹´ ì •ë³´ ì›Œí¬í”Œë¡œ ì‘ì„±

#### Workflow

1. `/pilot-pjt/workflow/subject3` ë””ë ‰í† ë¦¬ ë°‘ì— ì•„ë˜ 2ê°œ íŒŒì¼ ì¶”ê°€

   - `create_table_managed_smartcar_symptom_info.hql`
     ```sql
     create table if not exists Managed_SmartCar_Symptom_Info (
         car_number string,
         speed_p_avg string,
         speed_p_symptom string,
         break_p_avg string,
         break_p_symptom string,
         steer_a_cnt string,
         steer_p_symptom string,
         biz_date string
     )
     row format delimited
     fields terminated by ','
     stored as textfile;
     ```
   - `insert_table_managed_smartcar_symptom_info.hql`

     ```sql
     insert into table Managed_SmartCar_Symptom_Info
     select
         t1.car_number,
         t1.speed_p_avg_by_carnum,
         case
             when (abs((t1.speed_p_avg_by_carnum - t3.speed_p_avg) / t4.speed_p_std))  >  2
             then 'ë¹„ì •ìƒ'
             else   'ì •ìƒ'
         end
         as speed_p_symptom_score,
         t1.break_p_avg_by_carnum,
         case
             when (abs((t1.break_p_avg_by_carnum - t3.break_p_avg) / t4.break_p_std))  >  2
             then 'ë¹„ì •ìƒ'
             else   'ì •ìƒ'
         end
         as break_p_symptom_score,
         t2.steer_a_count,
         case
             when (t2.steer_a_count)  >   1000
             then 'ë¹„ì •ìƒ'
             else   'ì •ìƒ'
         end
         as steer_p_symptom_score,
         t1.biz_date
     from
         (select car_number, biz_date, avg(speed_pedal) as speed_p_avg_by_carnum, avg(break_pedal) as break_p_avg_by_carnum from managed_smartcar_drive_info where biz_date =  '${working_day}'  group by car_number, biz_date) t1
     join
         (select car_number, count(*) as steer_a_count from managed_smartcar_drive_info where steer_angle in ('L2','L3','R2','R3') and biz_date =  '${working_day}'  group by car_number) t2
     on
         t1.car_number = t2.car_number ,
         (select avg(speed_pedal) as speed_p_avg, avg(break_pedal) as break_p_avg from managed_smartcar_drive_info ) t3,
         (select stddev_pop(s.speed_p_avg_by_carnum) as speed_p_std, stddev_pop(s.break_p_avg_by_carnum) as break_p_std from
                     (select car_number, avg(speed_pedal) as speed_p_avg_by_carnum, avg(break_pedal) as break_p_avg_by_carnum from managed_smartcar_drive_info group by car_number) s) t4

     ```

2. Hue > Workflows > í¸ì§‘ê¸° > Workflow ë©”ë‰´ì—ì„œ ìƒì„± ë²„íŠ¼ í´ë¦­
3. ToolBoxì—ì„œ ì‘ì—… ì„ íƒ í›„ Hive Server2ì‘ì—…ì„ ì‘ì—… ë…¸ë“œì— Drag & Drop

   | ìˆœì„œ | íŒŒì¼ëª…                                           | ë§¤ê°œë³€ìˆ˜              |
   | ---- | ------------------------------------------------ | --------------------- |
   | 1    | `create_table_managed_smartcar_symptom_info.hql` | -                     |
   | 2    | `insert_table_managed_smartcar_symptom_info.hql` | working_day=\${today} |

   > ì‘ì—…ì„ ë°”ë¡œ ì‹¤í–‰í•˜ê³  ì‹¶ì„ë• ë§¤ê°œë³€ìˆ˜ë¥¼ ì˜¤ëŠ˜ë‚ ì§œë¡œ ì„¤ì •í•´ì£¼ë©´ ë¨ (`working_day=20190610`)

4. Workflow ì´ë¦„ì„ `Subject3-Workflow`ë¡œ ì„¤ì • í›„ ì €ì¥, ì œì¶œ ë²„íŠ¼ í´ë¦­

### Coordinator

1. `Hue > Workflows > í¸ì§‘ê¸° > Coordinator` ë©”ë‰´ì—ì„œ ìƒì„± ë²„íŠ¼ í´ë¦­
2. ì´ë¦„ì€ `Subject3-Coordinator` ë¡œ ì„¤ì •
3. ì›Œí¬í”Œë¡œëŠ” ì•ì„œ ì‘ì„±í–ˆë˜ `Subject3-Workflow` ì„ íƒ
4. ì‘ì—… ê°„ê²© ì„¤ì •
   | í•­ëª© | ì„¤ì • ê°’ |
   | -- | -- |
   | ì‹¤í–‰ ê°„ê²© | ë§¤ì¼, 02ì‹œ |
   | ì‹œì‘ ì¼ì | 2019ë…„ 6ì›” 9ì¼ 00ì‹œ 00ë¶„ |
   | ì¢…ë£Œ ì¼ì | 2019ë…„ 6ì›” 17ì¼ 23ì‹œ 59ë¶„ |
   | ì‹œê°„ëŒ€ | Asia/Seoul |
5. ë§¤ê°œë³€ìˆ˜ ì„¤ì • (`today`)
   ```
   ${coord:formatTime(coord:dateTzOffset(coord:nominalTime(), "Asia/Seoul"), 'yyyyMMdd')}
   ```
6. ì €ì¥ ë° ì œì¶œ
7. `Hue > Workflows > ëŒ€ì‹œë³´ë“œ > Coordinator` ì—ì„œ í™•ì¸

8. ì‘ì—…ì´ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ëë‹¤ë©´ Hive Editorì—ì„œ ì•„ë˜ ì¿¼ë¦¬ë¥¼ í†µí•´ í™•ì¸ ê°€ëŠ¥
   ```sql
   SELECT
   car_number,
   cast(speed_p_avg as int),
   speed_p_symptom,
   cast(break_p_avg as float),
   break_p_symptom,
   cast(steer_a_cnt as int),
   steer_p_symptom,
   biz_date
   FROM managed_smartcar_symptom_info
   where biz_date = '20190610'
   ```

## Subject4. ê¸´ê¸‰ ì ê²€ì´ í•„ìš”í•œ ìŠ¤ë§ˆíŠ¸ì¹´ ì •ë³´ ì›Œí¬í”Œë¡œ ì‘ì„±

#### Workflow

1. `/pilot-pjt/workflow/subject4` ë””ë ‰í† ë¦¬ ë°‘ì— ì•„ë˜ 2ê°œ íŒŒì¼ ì¶”ê°€

   - `create_table_managed_smartcar_emergency_check_info.hql`
     ```sql
     create table if not exists Managed_SmartCar_Emergency_Check_Info (
         car_number string,
         tire_check string,
         light_check string,
         engine_check string,
         break_check string,
         battery_check string,
         biz_date string
     )
     row format delimited
     fields terminated by ','
     stored as textfile;
     ```
   - `insert_table_managed_smartcar_emergency_check_info.hql`

     ```sql
     insert into table Managed_SmartCar_Emergency_Check_Info
     select

             t1.car_number,
             t2.symptom as tire_symptom,
             t3.symptom as light_symptom,
             t4.symptom as engine_symptom,
             t5.symptom as break_symptom,
             t6.symptom as battery_symptom,
             t1.biz_date
     from
             (select distinct car_number as car_number, biz_date from managed_smartcar_status_info  where biz_date = '${working_day}') t1

     left outer join ( select
                                 car_number,
                                 avg(tire_fl) as tire_fl_avg ,
                                 avg(tire_fr) as tire_fr_avg ,
                                 avg(tire_bl) as tire_bl_avg ,
                                 avg(tire_br) as tire_br_avg ,
                                 'íƒ€ì´ì–´ ì ê²€' as symptom
                         from managed_smartcar_status_info where biz_date ='${working_day}'
                         group by car_number
                         having  tire_fl_avg < 80 or tire_fr_avg < 80 or  tire_bl_avg < 80 or tire_br_avg < 80 ) t2
     on t1.car_number = t2.car_number

     left outer join ( select
                                     distinct car_number,
                                     'ë¼ì´íŠ¸ ì ê²€' as symptom
                         from managed_smartcar_status_info
                         where biz_date = '${working_day}' and (light_fl = '2' or light_fr = '2' or light_bl = '2' or light_br = '2')) t3
     on t1.car_number = t3.car_number

     left outer join ( select
                                 distinct car_number,
                                 'ì—”ì§„ ì ê²€' as symptom
                         from managed_smartcar_status_info
                         where biz_date = '${working_day}' and engine = 'C' ) t4
     on t1.car_number = t4.car_number

     left outer join ( select
                                 distinct car_number,
                                 'ë¸Œë ˆì´í¬ ì ê²€' as symptom
                         from managed_smartcar_status_info
                         where biz_date = '${working_day}' and break = 'C' ) t5

     on t1.car_number = t5.car_number

     left outer join (select
                                     car_number,
                                     avg(battery) as battery_avg,
                                     'ë°°í„°ë¦¬ ì ê²€' as symptom
                         from managed_smartcar_status_info where biz_date = '${working_day}'
                         group by car_number having battery_avg < 30 ) t6
     on t1.car_number = t6.car_number

     where t2.symptom is not null or t3.symptom is not null or t4.symptom is not null  or t5.symptom is not null  or t6.symptom is not null
     ```

2. Hue > Workflows > í¸ì§‘ê¸° > Workflow ë©”ë‰´ì—ì„œ ìƒì„± ë²„íŠ¼ í´ë¦­
3. ToolBoxì—ì„œ ì‘ì—… ì„ íƒ í›„ Hive Server2ì‘ì—…ì„ ì‘ì—… ë…¸ë“œì— Drag & Drop

   | ìˆœì„œ | íŒŒì¼ëª…                                                   | ë§¤ê°œë³€ìˆ˜              |
   | ---- | -------------------------------------------------------- | --------------------- |
   | 1    | `create_table_managed_smartcar_emergency_check_info.hql` | -                     |
   | 2    | `insert_table_managed_smartcar_emergency_check_info.hql` | working_day=\${today} |

   > ì‘ì—…ì„ ë°”ë¡œ ì‹¤í–‰í•˜ê³  ì‹¶ì„ë• ë§¤ê°œë³€ìˆ˜ë¥¼ ì˜¤ëŠ˜ë‚ ì§œë¡œ ì„¤ì •í•´ì£¼ë©´ ë¨ (`working_day=20190610`)

4. Workflow ì´ë¦„ì„ `Subject4-Workflow`ë¡œ ì„¤ì • í›„ ì €ì¥, ì œì¶œ ë²„íŠ¼ í´ë¦­

### Coordinator

1. `Hue > Workflows > í¸ì§‘ê¸° > Coordinator` ë©”ë‰´ì—ì„œ ìƒì„± ë²„íŠ¼ í´ë¦­
2. ì´ë¦„ì€ `Subject4-Coordinator` ë¡œ ì„¤ì •
3. ì›Œí¬í”Œë¡œëŠ” ì•ì„œ ì‘ì„±í–ˆë˜ `Subject4-Workflow` ì„ íƒ
4. ì‘ì—… ê°„ê²© ì„¤ì •
   | í•­ëª© | ì„¤ì • ê°’ |
   | -- | -- |
   | ì‹¤í–‰ ê°„ê²© | ë§¤ì¼, 02ì‹œ |
   | ì‹œì‘ ì¼ì | 2019ë…„ 6ì›” 9ì¼ 00ì‹œ 00ë¶„ |
   | ì¢…ë£Œ ì¼ì | 2019ë…„ 6ì›” 17ì¼ 23ì‹œ 59ë¶„ |
   | ì‹œê°„ëŒ€ | Asia/Seoul |
5. ë§¤ê°œë³€ìˆ˜ ì„¤ì • (`today`)
   ```
   ${coord:formatTime(coord:dateTzOffset(coord:nominalTime(), "Asia/Seoul"), 'yyyyMMdd')}
   ```
6. ì €ì¥ ë° ì œì¶œ
7. `Hue > Workflows > ëŒ€ì‹œë³´ë“œ > Coordinator` ì—ì„œ í™•ì¸

8. ì‘ì—…ì´ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ëë‹¤ë©´ Hive Editorì—ì„œ ì•„ë˜ ì¿¼ë¦¬ë¥¼ í†µí•´ í™•ì¸ ê°€ëŠ¥
   ```sql
   select * from managed_smartcar_emergency_check_info where biz_date = '20190610'
   ```

## Subject5. ìŠ¤ë§ˆíŠ¸ì¹´ ìš´ì „ì ì°¨ëŸ‰ìš©í’ˆ êµ¬ë§¤ ì´ë ¥ ì •ë³´ ì›Œí¬í”Œë¡œ ì‘ì„±

#### Workflow

1. `/pilot-pjt/workflow/subject5` ë””ë ‰í† ë¦¬ ë°‘ì— ì•„ë˜ 3ê°œ íŒŒì¼ ì¶”ê°€

   - `create_table_managed_smartcar_item_buylist_info.hql`
     ```sql
     create table if not exists Managed_SmartCar_Item_BuyList_Info (
         car_number string,
         sex string,
         age string,
         marriage string,
         region string,
         job string,
         car_capacity string,
         car_year string,
         car_model string,
         item string,
         score string
     )
     partitioned by( biz_month string )
     row format delimited
     fields terminated by ','
     stored as textfile;
     ```
   - `insert_table_managed_smartcar_item_buylist_info.hql`

     ```sql
     set hive.exec.dynamic.partition=true;
     set hive.exec.dynamic.partition.mode=nonstrict;

     insert overwrite table Managed_SmartCar_Item_BuyList_Info partition(biz_month)
     select
         t1.car_number,
         t1.sex,
         t1.age,
         t1.marriage,
         t1.region,
         t1.job,
         t1.car_capacity,
         t1.car_year,
         t1.car_model,
         t2.item,
         t2.score,
         t2.month as biz_month
     from
     SmartCar_Master_Over18 t1 join SmartCar_Item_Buylist t2
     on
     t1.car_number = t2.car_number
     where
     t2.month = '201606'
     ```

   - `local_save_managed_smartcar_item_buylist_info.hql`
     ```sql
     insert overwrite local directory '/tmp/pilot-pjt/item-buy-list'
     ROW FORMAT DELIMITED
     FIELDS TERMINATED BY ','
     select car_number, concat_ws("," , collect_set(item))
     from managed_smartcar_item_buylist_info
     group by car_number
     ```

2. Hue > Workflows > í¸ì§‘ê¸° > Workflow ë©”ë‰´ì—ì„œ ìƒì„± ë²„íŠ¼ í´ë¦­
3. ToolBoxì—ì„œ ì‘ì—… ì„ íƒ í›„ Hive Server2ì‘ì—…ì„ ì‘ì—… ë…¸ë“œì— Drag & Drop

   | ìˆœì„œ | íŒŒì¼ëª…                                                | ë§¤ê°œë³€ìˆ˜ |
   | ---- | ----------------------------------------------------- | -------- |
   | 1    | `create_table_managed_smartcar_item_buylist_info.hql` | -        |
   | 2    | `insert_table_managed_smartcar_item_buylist_info.hql` | -        |
   | 3    | `local_save_managed_smartcar_item_buylist_info.hql`   | -        |

4. Workflow ì´ë¦„ì„ `Subject5-Workflow`ë¡œ ì„¤ì • í›„ ì €ì¥, ì œì¶œ ë²„íŠ¼ í´ë¦­

### Coordinator

1. `Hue > Workflows > í¸ì§‘ê¸° > Coordinator` ë©”ë‰´ì—ì„œ ìƒì„± ë²„íŠ¼ í´ë¦­
2. ì´ë¦„ì€ `Subject5-Coordinator` ë¡œ ì„¤ì •
3. ì›Œí¬í”Œë¡œëŠ” ì•ì„œ ì‘ì„±í–ˆë˜ `Subject5-Workflow` ì„ íƒ
4. ì‘ì—… ê°„ê²© ì„¤ì •
   | í•­ëª© | ì„¤ì • ê°’ |
   | -- | -- |
   | ì‹¤í–‰ ê°„ê²© | ë§¤ì¼, 02ì‹œ |
   | ì‹œì‘ ì¼ì | 2019ë…„ 6ì›” 9ì¼ 00ì‹œ 00ë¶„ |
   | ì¢…ë£Œ ì¼ì | 2019ë…„ 6ì›” 17ì¼ 23ì‹œ 59ë¶„ |
   | ì‹œê°„ëŒ€ | Asia/Seoul |
5. ë§¤ê°œë³€ìˆ˜ ì„¤ì • (`today`)
   ```
   ${coord:formatTime(coord:dateTzOffset(coord:nominalTime(), "Asia/Seoul"), 'yyyyMMdd')}
   ```
6. ì €ì¥ ë° ì œì¶œ
7. `Hue > Workflows > ëŒ€ì‹œë³´ë“œ > Coordinator` ì—ì„œ í™•ì¸

8. ì‘ì—…ì´ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ëë‹¤ë©´ Hive Editorì—ì„œ ì•„ë˜ ì¿¼ë¦¬ë¥¼ í†µí•´ í™•ì¸ ê°€ëŠ¥
   ```sql
   SELECT * FROM managed_smartcar_item_buylist_info where biz_month = '201606'
   ```
9. íŒŒì¼ í™•ì¸ (`root` ê³„ì •ìœ¼ë¡œ `node1`ì— ssh ì ‘ì†)
   ```bash
   more /tmp/pilot-pjt/item-buy-list/000000_0
   ```
   > í´ë”ë¥¼ ì‚­ì œí•˜ê³  ìƒì„±í•˜ëŠ” ì‘ì—…ì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— ëª¨ë“  ê¶Œí•œì„ ì¤„ ìˆ˜ ìˆëŠ” `tmp` í´ë” ë°‘ì— ì‘ì—…í•˜ë„ë¡ ì„¤ì •í•˜ì˜€ìŒ

# 7. ë¹…ë°ì´í„° ë¶„ì„

## Impala ì„¤ì¹˜

![](./images/bigdata_practice/8.PNG)

- Impala ì„œë¹„ìŠ¤ ì¶”ê°€ í›„ `CM í™ˆ > Hue > êµ¬ì„±` í™”ë©´ì—ì„œ Impala ì„œë¹„ìŠ¤ í•­ëª©ì„ Impalaë¡œ ì„ íƒ
- Hue Stale Deploy
- Hue > Query Editor ë°‘ì— Impala ì¶”ê°€ëœê±° í™•ì¸

## Sqoop ì„¤ì¹˜

![](./images/bigdata_practice/9.PNG)

## Zeppelin ì„¤ì¹˜

- Zeppelin ì„¤ì¹˜

  ```bash
  cd /home/pilot-pjt
  wget http://archive.apache.org/dist/zeppelin/zeppelin-0.6.2/zeppelin-0.6.2-bin-all.tgz
  tar -xvf zeppelin-0.6.2-bin-all.tgz

  cd /home/pilot-pjt/zeppelin-0.6.2-bin-all/conf/
  cp zeppelin-env.sh.template zeppelin-env.sh
  vi zeppelin-env.sh
  ########## zepplin-env.sh ì‹œì‘ ##########
  ...
  export JAVA_HOME=/usr/java/jdk1.7.0_67-cloudera
  ...
  export HADOOP_CONF_DIR=/etc/hadoop/conf
  ...
  ########## zepplin-env.sh ë ##########

  #í•˜ì´ë¸Œ ì„ì‹œ ë””ë ‰í† ë¦¬ ê¶Œí•œ ë³€ê²½
  chmod 777 /tmp/hive
  # í•˜ì´ë¸Œ ì„¤ì • íŒŒì¼ ë³µì‚¬
  cp /etc/hive/conf/hive-site.xml /home/pilot-pjt/zeppelin-0.6.2-bin-all/conf/

  cd /home/pilot-pjt/zeppelin-0.6.2-bin-all/conf/
  cp zeppelin-site.xml.template zeppelin-site.xml
  vi zeppelin-site.xml
  ########## zeppelin-site.xml ì‹œì‘ ##########
  <property>
      <name>zeppelin.server.port</name>
      <value>8081</value>
      <description>Server port.</description>
  </property>
  ########## zeppelin-site.xml ë ##########

  cd
  vi .bash_profile
  ########## .bash_profile ì‹œì‘ ##########
  ...
  PATH=$PATH:/home/pilot-pjt/zeppelin-0.6.2-bin-all/bin
  export PATH
  ########## .bash_profile ë ##########
  source .bash_profile
  zeppelin-daemon.sh start
  zeppelin-daemon.sh status
  tail -f /home/pilot-pjt/zeppelin-0.6.2-bin-all/logs/zeppelin-root-node1.log
  ```

## Impalaë¥¼ í™œìš©í•œ ë°ì´í„° ì‹¤ì‹œê°„ ë¶„ì„

- `Hue > Query Editors > Impala`
- Impala Editorì—ì„œ í•˜ì´ë¸Œ í…Œì´ë¸” ì¡°íšŒ
  ```sql
  --ìš´ì „ íŒ¨í„´ ìŠ¤ë§ˆíŠ¸ì¹´ ì •ë³´ ì¡°íšŒ
  select * from managed_smartcar_symptom_info where biz_date = '20190610'
  --ê¸´ê¸‰ ì ê²€ì´ í•„ìš”í•œ ìŠ¤ë§ˆíŠ¸ì¹´ ì¡°íšŒ
  select * from managed_smartcar_emergency_check_info where biz_date = '20190610'
  --ìŠ¤ë§ˆíŠ¸ì¹´ ì°¨ëŸ‰ìš”í’ˆ êµ¬ë§¤ ì´ë ¥ ì •ë³´ ì¡°íšŒ
  select * from managed_smartcar_item_buylist_info where biz_month = '201606'
  ```
- ì§€ì—­ë³„ í‰ê·  ì†ë„ê°€ ê°€ì¥ ë†’ì€ ìŠ¤ë§ˆíŠ¸ì¹´ ì¡°íšŒ
  ```sql
  select T2.area_number, T2.car_number, T2.speed_avg
  from (
      select
              T1.area_number,
              T1.car_number,
              T1.speed_avg,
              rank() over(partition by T1.area_number order by T1.speed_avg desc) as ranking
      from (
              select area_number, car_number, avg(cast(speed as int)) as speed_avg
              from  managed_smartcar_drive_info
              group by area_number, car_number
              ) T1
  ) T2
  where ranking = 1
  ```

## Zeppelinì„ ì´ìš©í•œ ì‹¤ì‹œê°„ ë¶„ì„

- `Zeppelin > Notebook > Create new note` ë©”ë‰´ ì„ íƒ
- NoteNameì€ `SmartCar-Project` ë¡œ ì…ë ¥ í›„ `CreateNote` ë²„íŠ¼ í´ë¦­
- zeppelin ì‚¬ìš©í•˜ê¸°

  ```bash
  %sh
  hdfs dfs -cat /user/hive/warehouse/managed_smartcar_drive_info/biz_date=20190610/* | head

  ---

  %spark
  val driveData = sc.textFile("hdfs://node1:8020/user/hive/warehouse/managed_smartcar_drive_info/biz_date=20190610/*")
  case class DriveInfo(car_num: String,        sex: String,            age: String,            marriage: String,
                      region: String,         job: String,            car_capacity: String,   car_year: String,
                      car_model: String,      speed_pedal: String,    break_pedal: String,    steer_angle: String,
                      direct_light: String,   speed: String,          area_num: String,       date: String)

  val drive = driveData.map(sd=>sd.split(",")).map(
                  sd=>DriveInfo(sd(0).toString, sd(1).toString, sd(2).toString, sd(3).toString,
                              sd(4).toString, sd(5).toString, sd(6).toString, sd(7).toString,
                              sd(8).toString, sd(9).toString, sd(10).toString,sd(11).toString,
                              sd(12).toString,sd(13).toString,sd(14).toString,sd(15).toString
          )
  )

  drive.toDF().registerTempTable("DriveInfo")

  ---

  %spark.sql
  select T1.area_num, T1.avg_speed
  from  (select area_num, avg(speed) as avg_speed
      from DriveInfo
      group by area_num
      ) T1
  order by T1.avg_speed desc

  ---

  # ë™ì  ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê·  ì†ë„ë¥¼ ë°”ê¿”ê°€ë©´ì„œ ê²°ê³¼ í™•ì¸
  %spark.sql
  select T1.area_num, T1.avg_speed
  from  (select area_num, avg(speed) as avg_speed
      from DriveInfo
      group by area_num
      having avg_speed >= ${AvgSpeed=60}
      ) T1
  order by T1.avg_speed desc
  ```

- ìŠ¤ì¼€ì¥´ëŸ¬ ê¸°ëŠ¥ë„ ìˆìŒ (ìƒë‹¨ ì‹œê³„ëª¨ì–‘ ì•„ì´ì½˜)

## Mahoutì„ ì´ìš©í•œ ë°ì´í„° ë§ˆì´ë‹

> Spark MLë¡œ ëŒ€ì²´
